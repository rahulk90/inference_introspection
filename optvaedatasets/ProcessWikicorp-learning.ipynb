{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process WikiCorp Dataset\n",
    "    * Use this for learning [Features chosen based on frequency only]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import inflect,os,sys \n",
    "from utils.sparse_utils import saveSparseHDF5\n",
    "from utils.misc import savePickle, saveHDF5\n",
    "p = inflect.engine()\n",
    "suffix = '-split_hyphen'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from utils.sparse_utils import loadSparseHDF5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2001133\n"
     ]
    }
   ],
   "source": [
    "#Load wikicorp vocab\n",
    "assert os.path.exists('./wikicorp/WestburyLab.wikicorp.201004'+suffix+'.feat'),'Feature file not found'\n",
    "with open('./wikicorp/WestburyLab.wikicorp.201004'+suffix+'.feat','r') as f:\n",
    "    vocab = [k.strip().split(' ')[0] for k in f.readlines()]\n",
    "print len(vocab)\n",
    "vocab_arr = np.array(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2001133,)\n"
     ]
    }
   ],
   "source": [
    "#The number of singular nouns (flags->flag)\n",
    "vlist  = []\n",
    "w2idx  = {}\n",
    "\n",
    "for idx,v in enumerate(vocab):\n",
    "    sv = p.singular_noun(v)\n",
    "    if sv:\n",
    "        vlist.append(sv)\n",
    "        w2idx[sv] = idx\n",
    "    else:\n",
    "        vlist.append(v)\n",
    "        w2idx[v]  = idx\n",
    "    \n",
    "vocab_singular_only = set(vlist)\n",
    "vocab_singular_list = np.array(vlist)\n",
    "print vocab_singular_list.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2001133,)\n"
     ]
    }
   ],
   "source": [
    "data = loadSparseHDF5('dataset','./wikicorp/WestburyLab.wikicorp.201004'+suffix+'.h5')\n",
    "counts = np.array(data.sum(0)).squeeze().astype(int)\n",
    "print counts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   2263    2263    2263 ..., 1170157 1341684 1949093]\n",
      "20000 1999964 165 2001133\n"
     ]
    }
   ],
   "source": [
    "MAXVOCAB   = 20000\n",
    "sorted_idx = list(set(np.argsort(counts)[-MAXVOCAB:].tolist()))\n",
    "print np.sort(counts[sorted_idx])\n",
    "\n",
    "print  len(sorted_idx),np.max(sorted_idx),np.min(sorted_idx),len(vocab)\n",
    "#Add vectors corresponding to embedding words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3489\n"
     ]
    }
   ],
   "source": [
    "#Number of words we're double counting ~ 4k should be OK\n",
    "subset_w = [vocab[i] for i in sorted_idx]\n",
    "dblct    = []\n",
    "for w in subset_w:\n",
    "    if p.singular_noun(w) in subset_w:\n",
    "        dblct.append(w)\n",
    "print len(dblct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_subset = data.tocsc()[:,sorted_idx].tocsr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features_subset = [vocab[k] for k in sorted_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features_subset_singular = []\n",
    "for w in features_subset:\n",
    "    kk = p.singular_noun(w)\n",
    "    if kk:\n",
    "        features_subset_singular.append(kk)\n",
    "    else:\n",
    "        features_subset_singular.append(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features_subset          = np.array(features_subset)\n",
    "features_subset_singular = np.array(features_subset_singular)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2702.0 (3035070, 20000)\n",
      "(1229677,)\n",
      "(1229677, 20000)\n"
     ]
    }
   ],
   "source": [
    "#csr_matrix((data, indices, indptr), shape=(3, 3)).toarray()\n",
    "print data_subset.max(),data_subset.shape\n",
    "doccts = data_subset.max(1).toarray().squeeze()\n",
    "docs_keep_idx = np.where(doccts>5)[0]\n",
    "print docs_keep_idx.shape\n",
    "\n",
    "data_subset_minlen = data_subset[docs_keep_idx]\n",
    "print data_subset_minlen.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  7.00000000e+00,   7.00000000e+00,   7.00000000e+00, ...,\n",
       "         1.85590000e+04,   1.85600000e+04,   2.63520000e+04])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sort(np.array(data_subset_minlen.sum(1)).squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'loadHDF5' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-6d1677847234>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0midx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m]\u001b[0m   \u001b[0;34m=\u001b[0m \u001b[0mshufidx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m12000\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0msaveHDF5\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'wikicorp/idx-learning.h5'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0midx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloadHDF5\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'wikicorp/idx-learning.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mtrain_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'valid'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'test'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'loadHDF5' is not defined"
     ]
    }
   ],
   "source": [
    "#Restrict the documents\n",
    "\n",
    "if not os.path.exists('wikicorp/idx-learning.h5'):\n",
    "    np.random.seed(1)\n",
    "    shufidx = np.random.permutation(data_subset_minlen.shape[0])\n",
    "    idx = {}\n",
    "    idx['test']    = shufidx[:10000]\n",
    "    idx['valid']   = shufidx[10000:12000]\n",
    "    idx['train']   = shufidx[12000:]\n",
    "    saveHDF5('wikicorp/idx-learning.h5',idx)\n",
    "idx = loadHDF5('wikicorp/idx-learning.h5')\n",
    "train_idx, valid_idx, test_idx = idx['train'], idx['valid'], idx['test']\n",
    "\n",
    "\n",
    "TRAIN = data_subset_minlen[train_idx]\n",
    "VALID = data_subset_minlen[valid_idx]\n",
    "TEST  = data_subset_minlen[test_idx]\n",
    "print TRAIN.shape, VALID.shape, TEST.shape\n",
    "print np.sort(np.array(TRAIN.sum(1)).squeeze()).astype(int), np.sort(np.array(VALID.sum(1)).squeeze()), np.sort(np.array(TEST.sum(1)).squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved  3  objects\n"
     ]
    }
   ],
   "source": [
    "os.system('rm -rf ./wikicorp/data-learning.h5 ./wikicorp/misc-learning.pkl')\n",
    "saveSparseHDF5(TRAIN, 'train', './wikicorp/data-learning.h5')\n",
    "saveSparseHDF5(VALID, 'valid', './wikicorp/data-learning.h5')\n",
    "saveSparseHDF5(TEST,  'test' , './wikicorp/data-learning.h5')\n",
    "savePickle([{},features_subset,features_subset_singular],'./wikicorp/misc-learning.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
