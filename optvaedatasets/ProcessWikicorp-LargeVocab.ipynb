{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process WikiCorp Dataset to have a large vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import inflect,os,sys,re\n",
    "from utils.sparse_utils import saveSparseHDF5\n",
    "from utils.misc import savePickle\n",
    "p = inflect.engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from utils.sparse_utils import loadSparseHDF5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading...\n",
      "Read  1  objects\n",
      "Read  1  objects\n",
      "Read  1  objects\n",
      "Read  1  objects\n",
      "171834  words needed\n"
     ]
    }
   ],
   "source": [
    "from load import loadDataset\n",
    "dset1 = loadDataset('scws')\n",
    "dset2 = loadDataset('wordsim353')\n",
    "dset3 = loadDataset('imdb')\n",
    "dset4 = loadDataset('sst_fine')\n",
    "dset5 = loadDataset('rotten_tomatoes')\n",
    "dset6 = loadDataset('sst_binary')\n",
    "def wParse(k):\n",
    "    ws = re.sub('[^\\w\\s]','', k.strip())\n",
    "    ws = ws.lower()\n",
    "    return ws\n",
    "vocab_emb = set([wParse(dset1[k]['w1']) for k in dset1]+[wParse(dset1[k]['w2']) for k in dset1]\n",
    "               +[wParse(k) for k in dset3['word2idx'].keys()]+[wParse(k) for k in dset4['word2idx'].keys()]\n",
    "               +[wParse(k) for k in dset5['word2idx'].keys()])\n",
    "print len(vocab_emb),' words needed'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print dset6['word2idx'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#print set(dset6['word2idx'].keys()).difference(set(dset5['word2idx'].keys()))\n",
    "v_imdb = dset3['word2idx'].keys()\n",
    "v_sst = dset4['word2idx'].keys()\n",
    "v_rt = dset5['word2idx'].keys()\n",
    "def cleanVocab(word):\n",
    "    ws  = re.sub('[^\\w\\s-]','', word.strip())\n",
    "    return ws\n",
    "newvocab= set()\n",
    "for w in v_imdb:\n",
    "    neww = cleanVocab(w)\n",
    "    if neww in newvocab:\n",
    "        print w,neww9\n",
    "    else:\n",
    "        newvocab.add(neww)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print v_sst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2002178\n"
     ]
    }
   ],
   "source": [
    "#Load wikicorp vocab\n",
    "assert os.path.exists('./wikicorp/WestburyLab.wikicorp.201004.feat'),'Feature file not found'\n",
    "with open('./wikicorp/WestburyLab.wikicorp.201004.feat','r') as f:\n",
    "    vocab = [k.strip().split(' ')[0] for k in f.readlines()]\n",
    "print len(vocab)\n",
    "vocab_arr = np.array(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2002178,)\n"
     ]
    }
   ],
   "source": [
    "#The number of singular nouns (flags->flag)\n",
    "vlist  = []\n",
    "w2idx  = {}\n",
    "\n",
    "for idx,v in enumerate(vocab):\n",
    "    sv = p.singular_noun(v)\n",
    "    if sv:\n",
    "        vlist.append(sv)\n",
    "        w2idx[sv] = idx\n",
    "    else:\n",
    "        vlist.append(v)\n",
    "        w2idx[v]  = idx\n",
    "    \n",
    "vocab_singular_only = set(vlist)\n",
    "vocab_singular_list = np.array(vlist)\n",
    "print vocab_singular_list.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " selfexplanatorywhich dahh shaudenfraud ultimatley ctomvelu mooded kalithis loopits zannettis awakecutting colorbeautiful bareaed thoughshould 8mms backmight discussionquestion 71202 bravora yadayadayada hoitytoityness iveneverusedmyvoicebefore howinthehell systemswonderful believabilitythe lyonlisa jewswhen cheezinfested 270 274 275 sh1t actressesits mdmireles1295 noirthe backthats asleepit originalitynot oneher fistfulyojimbo azariawhats producermoody oppositejust oscarhungry captaincyborgmad somedaybut dafoethats nrashootemup tootling anycow kibitzergeneral etherything installmentkilljoy ultralovely rightincluding e7 murphywhen damonjude 1100ad exviet horrornongraphical e3 enchelada supermother trelkovski drymouthed 7ft gigglefits akanightmare lifede goodwe postspielberg suspensethen monoduologues friendsitll moviesnowadays ludicrosity listenit casteffectsboredomexcitementand monstersplatting familiarness curlywannabegonecompletelywrong utterancestrek bsgat paladinwho okcould modes1 epyonand powerfulemotional stationsofthecross israelisjust whoopdedoodles mugmy dirjohn"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-f734de8c1ebc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[0midxlst_vocab\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m==\u001b[0m\u001b[0mvocab_arr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[1;31m#assert len(idxlst_vocab)==1,'nsd'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m         \u001b[0mmapIdToIdx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'v_'\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0midxlst_vocab\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m         \u001b[0mfrom_vocab\u001b[0m  \u001b[1;33m+=\u001b[0m \u001b[0midxlst_vocab\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mw\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mvocab_singular_only\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "#Find all the words they map to\n",
    "from_vocab = []\n",
    "from_vocab_singular = []\n",
    "absent     = []\n",
    "mapIdToIdx = {}\n",
    "for w in vocab_emb:\n",
    "    if w in vocab:\n",
    "        idxlst_vocab = np.where(w==vocab_arr)[0].tolist()\n",
    "        #assert len(idxlst_vocab)==1,'nsd'\n",
    "        mapIdToIdx[w] = 'v_'+str(idxlst_vocab[0])\n",
    "        from_vocab  += idxlst_vocab\n",
    "    elif w in vocab_singular_only:\n",
    "        idxlst = np.where(w==vocab_singular_list)[0].tolist()\n",
    "        #assert len(idxlst)==1,'nsd'\n",
    "        from_vocab_singular+=idxlst\n",
    "        mapIdToIdx[w] = 's_'+str(idxlst[0])\n",
    "    else:\n",
    "        print w,\n",
    "        absent.append(w)\n",
    "print ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print len(set(from_vocab))\n",
    "print len(set(from_vocab_singular))\n",
    "print len(set(from_vocab+from_vocab_singular))\n",
    "idx_to_preserve = from_vocab+from_vocab_singular\n",
    "print 'Preserving ',len(idx_to_preserve),' words from original vocab'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = loadSparseHDF5('dataset','./wikicorp/WestburyLab.wikicorp.201004.h5')\n",
    "counts = np.array(data.sum(0)).squeeze().astype(int)\n",
    "print counts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print 'See how frequent the words are: ',[counts[k] for k in idx_to_preserve]\n",
    "print counts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "MAXVOCAB   = 20000\n",
    "sorted_idx = list(set(np.argsort(counts)[-MAXVOCAB:].tolist() + idx_to_preserve))\n",
    "print np.sort(counts[sorted_idx])\n",
    "\n",
    "print  len(sorted_idx),np.max(sorted_idx),np.min(sorted_idx),len(vocab)\n",
    "#Add vectors corresponding to embedding words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Check absentees\n",
    "aidx = []\n",
    "for idx in idx_to_preserve: \n",
    "    if idx not in sorted_idx:\n",
    "        aidx.append(idx)\n",
    "print len(aidx),len(idx_to_preserve)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Number of words we're double counting ~ 4k should be OK\n",
    "subset_w = [vocab[i] for i in sorted_idx]\n",
    "dblct    = []\n",
    "for w in subset_w:\n",
    "    if p.singular_noun(w) in subset_w:\n",
    "        dblct.append(w)\n",
    "print len(dblct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_subset = data.tocsc()[:,sorted_idx].tocsr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "features_subset = [vocab[k] for k in sorted_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features_subset_singular = []\n",
    "for w in features_subset:\n",
    "    kk = p.singular_noun(w)\n",
    "    if kk:\n",
    "        features_subset_singular.append(kk)\n",
    "    else:\n",
    "        features_subset_singular.append(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features_subset          = np.array(features_subset)\n",
    "features_subset_singular = np.array(features_subset_singular)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Check that all the embeddings are availdble here (Except 3)\n",
    "#Find all the words they map to\n",
    "mapIdx = {}\n",
    "for w in vocab_emb:\n",
    "    if w in features_subset:\n",
    "        idxlst_vocab = np.where(w==features_subset)[0].tolist()\n",
    "        assert len(idxlst_vocab)==1,'nsd'\n",
    "        mapIdx[w] = idxlst_vocab[0]\n",
    "    elif w in features_subset_singular:\n",
    "        idxlst = np.where(w==features_subset_singular)[0].tolist()\n",
    "        assert len(idxlst)==1,'nsd'\n",
    "        mapIdx[w] = idxlst[0]\n",
    "    else:\n",
    "        print w, 'not found'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#csr_matrix((data, indices, indptr), shape=(3, 3)).toarray()\n",
    "print data_subset.max(),data_subset.shape\n",
    "doccts = data_subset.max(1).toarray().squeeze()\n",
    "docs_keep_idx = np.where(doccts>5)[0]\n",
    "print docs_keep_idx.shape\n",
    "\n",
    "data_subset_minlen = data_subset[docs_keep_idx]\n",
    "print data_subset_minlen.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.sort(np.array(data_subset_minlen.sum(1)).squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Restrict the documents\n",
    "np.random.seed(1)\n",
    "shufidx = np.random.permutation(data_subset_minlen.shape[0])\n",
    "\n",
    "test_idx    = shufidx[:10000]\n",
    "valid_idx   = shufidx[10000:12000]\n",
    "train_idx   = shufidx[12000:]\n",
    "\n",
    "TRAIN = data_subset_minlen[train_idx]\n",
    "VALID = data_subset_minlen[valid_idx]\n",
    "TEST  = data_subset_minlen[test_idx]\n",
    "print TRAIN.shape, VALID.shape, TEST.shape\n",
    "print np.sort(np.array(TRAIN.sum(1)).squeeze()).astype(int), np.sort(np.array(VALID.sum(1)).squeeze()), np.sort(np.array(TEST.sum(1)).squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cts_train = np.array(TRAIN.sum(0)).squeeze()\n",
    "for k in mapIdx.values():\n",
    "    if cts_train[k]<2:\n",
    "        print features_subset[k],features_subset_singular[k],cts_train[k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "os.system('rm -rf ./wikicorp/data.h5 ./wikicorp/misc.pkl')\n",
    "saveSparseHDF5(TRAIN, 'train', './wikicorp/data.h5')\n",
    "saveSparseHDF5(VALID, 'valid', './wikicorp/data.h5')\n",
    "saveSparseHDF5(TEST,  'test' , './wikicorp/data.h5')\n",
    "savePickle([mapIdx,features_subset,features_subset_singular],'./wikicorp/misc.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
