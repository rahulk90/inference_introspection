{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process WikiCorp Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import inflect,os,sys\n",
    "from utils.sparse_utils import saveSparseHDF5\n",
    "from utils.misc import savePickle\n",
    "p = inflect.engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from utils.sparse_utils import loadSparseHDF5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1728  words needed\n"
     ]
    }
   ],
   "source": [
    "from load import loadDataset\n",
    "dset1 = loadDataset('scws')\n",
    "dset2 = loadDataset('wordsim353')\n",
    "def wParse(k):\n",
    "    return k.lower().replace('-','')\n",
    "vocab_emb = set([wParse(dset1[k]['w1']) for k in dset1]+[wParse(dset1[k]['w2']) for k in dset1]+\n",
    "            [wParse(dset2[k]['w1']) for k in dset2]+[wParse(dset2[k]['w2']) for k in dset2])\n",
    "print len(vocab_emb),' words needed'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Feature file not found",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-161c1959d44e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#Load wikicorp vocab\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32massert\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'./wikicorp/smallVocab/WestburyLab.wikicorp.201004.feat'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'Feature file not found'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'./wikicorp/smallVocab/WestburyLab.wikicorp.201004.feat'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'r'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mvocab\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m' '\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadlines\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mprint\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvocab\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAssertionError\u001b[0m: Feature file not found"
     ]
    }
   ],
   "source": [
    "#Load wikicorp vocab\n",
    "assert os.path.exists('./wikicorp/smallVocab/WestburyLab.wikicorp.201004.feat'),'Feature file not found'\n",
    "with open('./wikicorp/smallVocab/WestburyLab.wikicorp.201004.feat','r') as f:\n",
    "    vocab = [k.strip().split(' ')[0] for k in f.readlines()]\n",
    "print len(vocab)\n",
    "vocab_arr = np.array(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#The number of singular nouns (flags->flag)\n",
    "vlist  = []\n",
    "w2idx  = {}\n",
    "\n",
    "for idx,v in enumerate(vocab):\n",
    "    sv = p.singular_noun(v)\n",
    "    if sv:\n",
    "        vlist.append(sv)\n",
    "        w2idx[sv] = idx\n",
    "    else:\n",
    "        vlist.append(v)\n",
    "        w2idx[v]  = idx\n",
    "    \n",
    "vocab_singular_only = set(vlist)\n",
    "vocab_singular_list = np.array(vlist)\n",
    "print vocab_singular_list.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Find all the words they map to\n",
    "from_vocab = []\n",
    "from_vocab_singular = []\n",
    "absent     = []\n",
    "mapIdToIdx = {}\n",
    "for w in vocab_emb:\n",
    "    if w in vocab:\n",
    "        idxlst_vocab = np.where(w==vocab_arr)[0].tolist()\n",
    "        assert len(idxlst_vocab)==1,'nsd'\n",
    "        mapIdToIdx[w] = 'v_'+str(idxlst_vocab[0])\n",
    "        from_vocab  += idxlst_vocab\n",
    "    elif w in vocab_singular_only:\n",
    "        idxlst = np.where(w==vocab_singular_list)[0].tolist()\n",
    "        assert len(idxlst)==1,'nsd'\n",
    "        from_vocab_singular+=idxlst\n",
    "        mapIdToIdx[w] = 's_'+str(idxlst[0])\n",
    "    else:\n",
    "        print w, 'not found'\n",
    "        absent.append(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print len(set(from_vocab))\n",
    "print len(set(from_vocab_singular))\n",
    "print len(set(from_vocab+from_vocab_singular))\n",
    "idx_to_preserve = from_vocab+from_vocab_singular\n",
    "print 'Preserving ',len(idx_to_preserve),' words from original vocab'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = loadSparseHDF5('dataset','./wikicorp/smallVocab/WestburyLab.wikicorp.201004.h5')\n",
    "counts = np.array(data.sum(0)).squeeze().astype(int)\n",
    "print counts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print 'See how frequent the words are: ',[counts[k] for k in idx_to_preserve]\n",
    "print counts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "MAXVOCAB   = 20000\n",
    "sorted_idx = list(set(np.argsort(counts)[-MAXVOCAB:].tolist() + idx_to_preserve))\n",
    "print np.sort(counts[sorted_idx])\n",
    "\n",
    "print  len(sorted_idx),np.max(sorted_idx),np.min(sorted_idx),len(vocab)\n",
    "#Add vectors corresponding to embedding words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Check absentees\n",
    "aidx = []\n",
    "for idx in idx_to_preserve: \n",
    "    if idx not in sorted_idx:\n",
    "        aidx.append(idx)\n",
    "print len(aidx),len(idx_to_preserve)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Number of words we're double counting ~ 4k should be OK\n",
    "subset_w = [vocab[i] for i in sorted_idx]\n",
    "dblct    = []\n",
    "for w in subset_w:\n",
    "    if p.singular_noun(w) in subset_w:\n",
    "        dblct.append(w)\n",
    "print len(dblct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_subset = data.tocsc()[:,sorted_idx].tocsr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "features_subset = [vocab[k] for k in sorted_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features_subset_singular = []\n",
    "for w in features_subset:\n",
    "    kk = p.singular_noun(w)\n",
    "    if kk:\n",
    "        features_subset_singular.append(kk)\n",
    "    else:\n",
    "        features_subset_singular.append(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features_subset          = np.array(features_subset)\n",
    "features_subset_singular = np.array(features_subset_singular)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Check that all the embeddings are availdble here (Except 3)\n",
    "#Find all the words they map to\n",
    "mapIdx = {}\n",
    "for w in vocab_emb:\n",
    "    if w in features_subset:\n",
    "        idxlst_vocab = np.where(w==features_subset)[0].tolist()\n",
    "        assert len(idxlst_vocab)==1,'nsd'\n",
    "        mapIdx[w] = idxlst_vocab[0]\n",
    "    elif w in features_subset_singular:\n",
    "        idxlst = np.where(w==features_subset_singular)[0].tolist()\n",
    "        assert len(idxlst)==1,'nsd'\n",
    "        mapIdx[w] = idxlst[0]\n",
    "    else:\n",
    "        print w, 'not found'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#csr_matrix((data, indices, indptr), shape=(3, 3)).toarray()\n",
    "print data_subset.max(),data_subset.shape\n",
    "doccts = data_subset.max(1).toarray().squeeze()\n",
    "docs_keep_idx = np.where(doccts>5)[0]\n",
    "print docs_keep.shape\n",
    "\n",
    "data_subset_minlen = data_subset[docs_keep]\n",
    "print data_subset_minlen.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Restrict the documents\n",
    "np.random.seed(1)\n",
    "shufidx = np.random.permutation(data_subset_minlen.shape[0])\n",
    "\n",
    "test_idx    = shufidx[:1000]\n",
    "train_idx   = shufidx[1000:]\n",
    "\n",
    "TRAIN = data_subset_minlen[train_idx]\n",
    "VALID = data_subset_minlen[test_idx]\n",
    "TEST  = data_subset_minlen[test_idx]\n",
    "print TRAIN.shape, VALID.shape, TEST.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cts_train = np.array(TRAIN.sum(0)).squeeze()\n",
    "for k in mapIdx.values():\n",
    "    if cts_train[k]<2:\n",
    "        print features_subset[k],features_subset_singular[k],cts_train[k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "os.system('rm -rf ./wikicorp/data.h5 ./wikicorp/misc.pkl')\n",
    "saveSparseHDF5(TRAIN, 'train', './wikicorp/data.h5')\n",
    "saveSparseHDF5(VALID, 'valid', './wikicorp/data.h5')\n",
    "saveSparseHDF5(TEST,  'test' , './wikicorp/data.h5')\n",
    "savePickle([mapIdx,features_subset,features_subset_singular],'./wikicorp/misc.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
